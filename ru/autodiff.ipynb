{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Tce3stUlHN0L"
      },
      "source": [
        "##### Авторские права 2020 Авторы TensorFlow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
        },
        "colab_type": "code",
        "id": "tuOe1ymfHZPu"
      },
      "outputs": [

      ],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you erwefw may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qFdPvlXBOdUN"
      },
      "source": [
        "# Введение в градиенты и автоматическую дифференциацию"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MfBg1C5NB3X0"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td><a target=\"_blank\" href=\"https://www.tensorflow.org/guide/autodiff\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">Посмотреть на TensorFlow.org</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/guide/autodiff.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Запускаем в Google Colab</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/guide/autodiff.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">Посмотреть исходный код на GitHub</a></td>\n",
        "  <td><a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/guide/autodiff.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Скачать блокнот</a></td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "r6P32iYYV27b"
      },
      "source": [
        "## Автоматическая дифференциация и градиенты\n",
        "\n",
        "[Автоматическое дифференцирование](https://en.wikipedia.org/wiki/Automatic_differentiation) полезно для реализации алгоритмов машинного обучения, таких как [обратное распространение,](https://en.wikipedia.org/wiki/Backpropagation) для обучения нейронных сетей.\n",
        "\n",
        "В этом руководстве мы обсудим способы вычисления градиентов с помощью TensorFlow, особенно при активном выполнении."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MUXex9ctTuDB"
      },
      "source": [
        "## Настроить"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "IqR2PQG4ZaZ0"
      },
      "outputs": [

      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xHxb-dlhMIzW"
      },
      "source": [
        "## Вычислительные градиенты\n",
        "\n",
        "Чтобы различать автоматически, TensorFlow должен помнить, какие операции в каком порядке выполняются во время *прямого* прохода. Затем во время *обратного прохода* TensorFlow просматривает этот список операций в обратном порядке для вычисления градиентов."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1CLWJl0QliB0"
      },
      "source": [
        "## Ленты с градиентом\n",
        "\n",
        "TensorFlow предоставляет [tf.GradientTape](https://www.tensorflow.org/api_docs/python/tf/GradientTape) API для автоматического различения; то есть вычисление градиента вычисления относительно некоторых входных данных, обычно `tf.Variable` s. TensorFlow «записывает» соответствующие операции, выполняемые в контексте `tf.GradientTape` на «ленту». Затем TensorFlow использует эту ленту для вычисления градиентов «записанного» вычисления с использованием [обратного дифференцирования](https://en.wikipedia.org/wiki/Automatic_differentiation) .\n",
        "\n",
        "Вот простой пример:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "Xq9GgTCP7a4A"
      },
      "outputs": [

      ],
      "source": [
        "x = tf.Variable(3.0)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  y = x**2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CR9tFAP_7cra"
      },
      "source": [
        "После того, как вы записали некоторые операции, используйте `GradientTape.gradient(target, sources)` чтобы вычислить градиент некоторой цели (часто потери) относительно некоторого источника (часто переменных модели)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "LsvrwF6bHroC"
      },
      "outputs": [

      ],
      "source": [
        "# dy = 2x * dx\n",
        "dy_dx = tape.gradient(y, x)\n",
        "dy_dx.numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Q2_aqsO25Vx1"
      },
      "source": [
        "В приведенном выше примере используются скаляры, но `tf.GradientTape` так же легко работает с любым тензором:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "vacZ3-Ws5VdV"
      },
      "outputs": [

      ],
      "source": [
        "w = tf.Variable(tf.random.normal((3, 2)), name='w')\n",
        "b = tf.Variable(tf.zeros(2, dtype=tf.float32), name='b')\n",
        "x = [[1., 2., 3.]]\n",
        "\n",
        "with tf.GradientTape(persistent=True) as tape:\n",
        "  y = x @ w + b\n",
        "  loss = tf.reduce_mean(y**2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "i4eXOkrQ-9Pb"
      },
      "source": [
        "Чтобы получить градиент `y` по обеим переменным, вы можете передать их в качестве источников в метод `gradient` . Лента гибкая в отношении того, как передаются источники, и принимает любую вложенную комбинацию списков или словарей и возвращает градиент, структурированный таким же образом (см. `tf.nest` )."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "luOtK1Da_BR0"
      },
      "outputs": [

      ],
      "source": [
        "[dl_dw, dl_db] = tape.gradient(loss, [w, b])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ei4iVXi6qgM7"
      },
      "source": [
        "Градиент по отношению к каждому источнику имеет форму источника:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "aYbWRFPZqk4U"
      },
      "outputs": [

      ],
      "source": [
        "print(w.shape)\n",
        "print(dl_dw.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dI_SzxHsvao1"
      },
      "source": [
        "Вот снова расчет градиента, на этот раз передающий словарь переменных:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "d73cY6NOuaMd"
      },
      "outputs": [

      ],
      "source": [
        "my_vars = {\n",
        "    'w': tf.Variable(tf.random.normal((3, 2)), name='w'),\n",
        "    'b': tf.Variable(tf.zeros(2, dtype=tf.float32), name='b')\n",
        "}\n",
        "\n",
        "grad = tape.gradient(loss, my_vars)\n",
        "grad['b']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HZ2LvHifEMgO"
      },
      "source": [
        "## Градиенты по модели\n",
        "\n",
        "It's common to collect `tf.Variables` into a `tf.Module` or one of its subclasses (`layers.Layer`, `keras.Model`) for [checkpointing](checkpoint.ipynb) and [exporting](saved_model.ipynb).\n",
        "\n",
        "В большинстве случаев вам нужно рассчитывать градиенты относительно обучаемых переменных модели. Поскольку все подклассы `tf.Module` объединяют свои переменные в свойстве `Module.trainable_variables` , вы можете вычислить эти градиенты в нескольких строках кода: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "JvesHtbQESc-"
      },
      "outputs": [

      ],
      "source": [
        "layer = tf.keras.layers.Dense(2, activation='relu')\n",
        "x = tf.constant([[1., 2., 3.]])\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  # Forward pass\n",
        "  y = layer(x)\n",
        "  loss = tf.reduce_mean(y**2)\n",
        "\n",
        "# Calculate gradients with respect to every trainable variable\n",
        "grad = tape.gradient(loss, layer.trainable_variables)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "PR_ezr6UFrpI"
      },
      "outputs": [

      ],
      "source": [
        "for var, g in zip(layer.trainable_variables, grad):\n",
        "  print(f'{var.name}, shape: {g.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "f6Gx6LS714zR"
      },
      "source": [
        "<a id=\"watches\"></a>\n",
        "\n",
        "## Контроль того, что смотрит лента"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "N4VlqKFzzGaC"
      },
      "source": [
        "Поведение по умолчанию - записывать все операции после доступа к обучаемой `tf.Variable` . Причины этого:\n",
        "\n",
        "- Ленте необходимо знать, какие операции записывать при прямом проходе, чтобы вычислить градиенты при обратном проходе.\n",
        "- Лента содержит ссылки на промежуточные выходы, поэтому вы не хотите записывать ненужные операции.\n",
        "- Наиболее распространенный вариант использования включает вычисление градиента потерь по всем обучаемым переменным модели.\n",
        "\n",
        "Например, следующее не может вычислить градиент, потому что `tf.Tensor` не \"отслеживается\" по умолчанию, а `tf.Variable` не обучается:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "Kj9gPckdB37a"
      },
      "outputs": [

      ],
      "source": [
        "# A trainable variable\n",
        "x0 = tf.Variable(3.0, name='x0')\n",
        "# Not trainable\n",
        "x1 = tf.Variable(3.0, name='x1', trainable=False)\n",
        "# Not a Variable: A variable + tensor returns a tensor.\n",
        "x2 = tf.Variable(2.0, name='x2') + 1.0\n",
        "# Not a variable\n",
        "x3 = tf.constant(3.0, name='x3')\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  y = (x0**2) + (x1**2) + (x2**2)\n",
        "\n",
        "grad = tape.gradient(y, [x0, x1, x2, x3])\n",
        "\n",
        "for g in grad:\n",
        "  print(g)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RkcpQnLgNxgi"
      },
      "source": [
        "Вы можете перечислить переменные, за которыми следит лента, используя метод `GradientTape.watched_variables` :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "hwNwjW1eAkib"
      },
      "outputs": [

      ],
      "source": [
        "[var.name for var in tape.watched_variables()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NB9I1uFvB4tf"
      },
      "source": [
        "`tf.GradientTape` предоставляет перехватчики, которые дают пользователю контроль над тем, что смотрит, а что нет.\n",
        "\n",
        "Чтобы записать градиенты относительно `tf.Tensor` , вам нужно вызвать `GradientTape.watch(x)` :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "tVN1QqFRDHBK"
      },
      "outputs": [

      ],
      "source": [
        "x = tf.constant(3.0)\n",
        "with tf.GradientTape() as tape:\n",
        "  tape.watch(x)\n",
        "  y = x**2\n",
        "\n",
        "# dy = 2x * dx\n",
        "dy_dx = tape.gradient(y, x)\n",
        "print(dy_dx.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qxsiYnf2DN8K"
      },
      "source": [
        "И наоборот, чтобы отключить поведение по умолчанию для просмотра всех `tf.Variables` , установите `watch_accessed_variables=False` при создании градиентной ленты. Этот расчет использует две переменные, но связывает градиент только для одной из переменных:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "7QPzwWvSEwIp"
      },
      "outputs": [

      ],
      "source": [
        "x0 = tf.Variable(0.0)\n",
        "x1 = tf.Variable(10.0)\n",
        "\n",
        "with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
        "  tape.watch(x1)\n",
        "  y0 = tf.math.sin(x0)\n",
        "  y1 = tf.nn.softplus(x1)\n",
        "  y = y0 + y1\n",
        "  ys = tf.reduce_sum(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TRduLbE1H2IJ"
      },
      "source": [
        "Поскольку `GradientTape.watch` не был вызван на `x0` , градиент по нему не вычисляется:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "e6GM-3evH1Sz"
      },
      "outputs": [

      ],
      "source": [
        "# dy = 2x * dx\n",
        "grad = tape.gradient(ys, {'x0': x0, 'x1': x1})\n",
        "\n",
        "print('dy/dx0:', grad['x0'])\n",
        "print('dy/dx1:', grad['x1'].numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2g1nKB6P-OnA"
      },
      "source": [
        "## Промежуточные результаты\n",
        "\n",
        "Вы также можете запросить градиенты вывода относительно промежуточных значений, вычисленных в контексте `tf.GradientTape` ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "7XaPRAwUyYms"
      },
      "outputs": [

      ],
      "source": [
        "x = tf.constant(3.0)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  tape.watch(x)\n",
        "  y = x * x\n",
        "  z = y * y\n",
        "\n",
        "# Use the tape to compute the gradient of z with respect to the\n",
        "# intermediate value y.\n",
        "# dz_dx = 2 * y, where y = x ** 2\n",
        "print(tape.gradient(z, y).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ISkXuY7YzIcS"
      },
      "source": [
        "По умолчанию ресурсы, удерживаемые `GradientTape` , высвобождаются, как только вызывается метод `GradientTape.gradient()` . Чтобы вычислить несколько градиентов за одно вычисление, создайте ленту `persistent` градиента. Это позволяет многократно вызывать метод `gradient()` мере освобождения ресурсов при сборке мусора для объекта ленты. Например:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "zZaCm3-9zVCi"
      },
      "outputs": [

      ],
      "source": [
        "x = tf.constant([1, 3.0])\n",
        "with tf.GradientTape(persistent=True) as tape:\n",
        "  tape.watch(x)\n",
        "  y = x * x\n",
        "  z = y * y\n",
        "\n",
        "print(tape.gradient(z, x).numpy())  # 108.0 (4 * x**3 at x = 3)\n",
        "print(tape.gradient(y, x).numpy())  # 6.0 (2 * x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "j8bv_jQFg6CN"
      },
      "outputs": [

      ],
      "source": [
        "del tape   # Drop the reference to the tape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "O_ZY-9BUB7vX"
      },
      "source": [
        "## Примечания по производительности\n",
        "\n",
        "- Есть крошечные накладные расходы, связанные с выполнением операций внутри контекста градиентной ленты. Для наиболее энергичного выполнения это не будет заметной ценой, но вы все равно должны использовать ленточный контекст вокруг областей только там, где это необходимо.\n",
        "\n",
        "- Ленты с градиентом используют память для хранения промежуточных результатов, включая входные и выходные данные, для использования во время обратного прохода.\n",
        "\n",
        "    Для эффективности некоторым операциям (например, `ReLU` ) не нужно сохранять свои промежуточные результаты, и они удаляются во время прямого прохода. Однако, если вы используете на своей ленте `persistent=True` , *ничего не сбрасывается,* и ваше пиковое использование памяти будет выше."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9dLBpZsJebFq"
      },
      "source": [
        "## Градиенты нескалярных целей"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7pldU9F5duP2"
      },
      "source": [
        "По сути, градиент - это операция над скаляром."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "qI0sDV_WeXBb"
      },
      "outputs": [

      ],
      "source": [
        "x = tf.Variable(2.0)\n",
        "with tf.GradientTape(persistent=True) as tape:\n",
        "  y0 = x**2\n",
        "  y1 = 1 / x\n",
        "\n",
        "print(tape.gradient(y0, x).numpy())\n",
        "print(tape.gradient(y1, x).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "COEyYp34fxj4"
      },
      "source": [
        "Таким образом, если вы запросите градиент нескольких целей, результат для каждого источника будет:\n",
        "\n",
        "- Градиент суммы целей, или эквивалентно\n",
        "- Сумма градиентов каждой цели."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "o4a6_YOcfWKS"
      },
      "outputs": [

      ],
      "source": [
        "x = tf.Variable(2.0)\n",
        "with tf.GradientTape() as tape:\n",
        "  y0 = x**2\n",
        "  y1 = 1 / x\n",
        "\n",
        "print(tape.gradient({'y0': y0, 'y1': y1}, x).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uvP-mkBMgbym"
      },
      "source": [
        "Точно так же, если цель (и) не скалярна, градиент суммы вычисляется:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "DArPWqsSh5un"
      },
      "outputs": [

      ],
      "source": [
        "x = tf.Variable(2.)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  y = x * [3., 4.]\n",
        "\n",
        "print(tape.gradient(y, x).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "flDbx68Zh5Lb"
      },
      "source": [
        "Это упрощает получение градиента суммы совокупных потерь или градиента суммы поэлементного расчета потерь.\n",
        "\n",
        "Если вам нужен отдельный градиент для каждого элемента, см. [Якобианы](advanced_autodiff.ipynb#jacobians) ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iwFswok8RAly"
      },
      "source": [
        "В некоторых случаях якобиан можно пропустить. Для поэлементного вычисления градиент суммы дает производную каждого элемента по отношению к его входному элементу, поскольку каждый элемент независим:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "JQvk_jnMmTDS"
      },
      "outputs": [

      ],
      "source": [
        "x = tf.linspace(-10.0, 10.0, 200+1)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  tape.watch(x)\n",
        "  y = tf.nn.sigmoid(x)\n",
        "\n",
        "dy_dx = tape.gradient(y, x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "e_f2QgDPmcPE"
      },
      "outputs": [

      ],
      "source": [
        "plt.plot(x, y, label='y')\n",
        "plt.plot(x, dy_dx, label='dy/dx')\n",
        "plt.legend()\n",
        "_ = plt.xlabel('x')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6kADybtQzYj4"
      },
      "source": [
        "## Поток управления\n",
        "\n",
        "Поскольку ленты записывают операции по мере их выполнения, поток управления Python (например, с использованием `if` и `while` ) обрабатывается естественным образом.\n",
        "\n",
        "Здесь разные переменные используются в каждой ветви `if` . Градиент подключается только к той переменной, которая использовалась:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "ciFLizhrrjy7"
      },
      "outputs": [

      ],
      "source": [
        "x = tf.constant(1.0)\n",
        "\n",
        "v0 = tf.Variable(2.0)\n",
        "v1 = tf.Variable(2.0)\n",
        "\n",
        "with tf.GradientTape(persistent=True) as tape:\n",
        "  tape.watch(x)\n",
        "  if x > 0.0:\n",
        "    result = v0\n",
        "  else:\n",
        "    result = v1**2 \n",
        "\n",
        "dv0, dv1 = tape.gradient(result, [v0, v1])\n",
        "\n",
        "print(dv0)\n",
        "print(dv1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HKnLaiapsjeP"
      },
      "source": [
        "Просто помните, что сами управляющие операторы не дифференцируемы, поэтому они невидимы для оптимизаторов на основе градиентов.\n",
        "\n",
        "В зависимости от значения `x` в приведенном выше примере на ленте записывается либо `result = v0` либо `result = v1**2` . Градиент по `x` всегда равен `None` ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "8k05WmuAwPm7"
      },
      "outputs": [

      ],
      "source": [
        "dx = tape.gradient(result, x)\n",
        "\n",
        "print(dx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "egypBxISAHhx"
      },
      "source": [
        "## Получение градиента `None`\n",
        "\n",
        "Когда цель не подключена к источнику, вы получите градиент `None` .\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "CU185WDM81Ut"
      },
      "outputs": [

      ],
      "source": [
        "x = tf.Variable(2.)\n",
        "y = tf.Variable(3.)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  z = y * y\n",
        "print(tape.gradient(z, x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sZbKpHfBRJym"
      },
      "source": [
        "Здесь `z` , очевидно, не связан с `x` , но есть несколько менее очевидных способов отсоединения градиента."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eHDzDOiQ8xmw"
      },
      "source": [
        "### 1. Заменил переменную на тензор.\n",
        "\n",
        "В разделе [«Контроль за тем, что смотрит лента»](#watches) вы видели, что лента автоматически отслеживает `tf.Variable` но не `tf.Tensor` .\n",
        "\n",
        "Одна из распространенных ошибок - непреднамеренная замена `tf.Variable` на `tf.Tensor` вместо использования `Variable.assign` для обновления `tf.Variable` . Вот пример:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "QPKY4Tn9zX7_"
      },
      "outputs": [

      ],
      "source": [
        "x = tf.Variable(2.0)\n",
        "\n",
        "for epoch in range(2):\n",
        "  with tf.GradientTape() as tape:\n",
        "    y = x+1\n",
        "\n",
        "  print(type(x).__name__, \":\", tape.gradient(y, x))\n",
        "  x = x + 1   # This should be `x.assign_add(1)`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3gwZKxgA97an"
      },
      "source": [
        "### 2. Выполнял расчеты вне TensorFlow.\n",
        "\n",
        "Лента не может записать путь градиента, если вычисление выходит из TensorFlow. Например:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "jmoLCDJb_yw1"
      },
      "outputs": [

      ],
      "source": [
        "x = tf.Variable([[1.0, 2.0],\n",
        "                 [3.0, 4.0]], dtype=tf.float32)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  x2 = x**2\n",
        "\n",
        "  # This step is calculated with NumPy\n",
        "  y = np.mean(x2, axis=0)\n",
        "\n",
        "  # Like most ops, reduce_mean will cast the NumPy array to a constant tensor\n",
        "  # using `tf.convert_to_tensor`.\n",
        "  y = tf.reduce_mean(y, axis=0)\n",
        "\n",
        "print(tape.gradient(y, x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "p3YVfP3R-tp7"
      },
      "source": [
        "### 3. Перенести градиенты через целое число или строку.\n",
        "\n",
        "Целые числа и строки не дифференцируются. Если путь вычисления использует эти типы данных, градиент не будет.\n",
        "\n",
        "Никто не ожидает, что строки будут дифференцируемыми, но легко случайно создать константу или переменную `int` если вы не укажете `dtype` ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "9jlHXHqfASU3"
      },
      "outputs": [

      ],
      "source": [
        "# The x0 variable has an `int` dtype.\n",
        "x = tf.Variable([[2, 2],\n",
        "                 [2, 2]])\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  # The path to x1 is blocked by the `int` dtype here.\n",
        "  y = tf.cast(x, tf.float32)\n",
        "  y = tf.reduce_sum(x)\n",
        "\n",
        "print(tape.gradient(y, x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RsdP_mTHX9L1"
      },
      "source": [
        "TensorFlow не выполняет автоматическое преобразование типов между типами, поэтому на практике вы часто будете получать ошибку типа вместо отсутствующего градиента."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WyAZ7C8qCEs6"
      },
      "source": [
        "### 5. Получение градиентов через объект с отслеживанием состояния.\n",
        "\n",
        "Состояние останавливает градиенты. Когда вы читаете из объекта с сохранением состояния, лента может видеть только текущее состояние, а не историю, которая к нему ведет.\n",
        "\n",
        "`tf.Tensor` неизменен. Вы не можете изменить тензор после его создания. У него есть *значение* , но нет *состояния* . Все операции, описанные до сих пор, также не имеют состояния: вывод `tf.matmul` зависит только от его входных данных.\n",
        "\n",
        "`tf.Variable` имеет внутреннее состояние, свое значение. Когда вы используете переменную, считывается состояние. Вычисление градиента относительно переменной является нормальным явлением, но состояние переменной препятствует дальнейшему возврату вычислений градиента. Например:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "C1tLeeRFE479"
      },
      "outputs": [

      ],
      "source": [
        "x0 = tf.Variable(3.0)\n",
        "x1 = tf.Variable(0.0)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  # Update x1 = x1 + x0.\n",
        "  x1.assign_add(x0)\n",
        "  # The tape starts recording from x1.\n",
        "  y = x1**2   # y = (x1 + x0)**2\n",
        "\n",
        "# This doesn't work.\n",
        "print(tape.gradient(y, x0))   #dy/dx0 = 2*(x1 + x2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xKA92-dqF2r-"
      },
      "source": [
        "Аналогично итераторы `tf.data.Dataset` и `tf.queue` сохраняют состояние и останавливают все градиенты на тензорах, которые проходят через них."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HHvcDGIbOj2I"
      },
      "source": [
        "## Градиент не зарегистрирован"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aoc-A6AxVqry"
      },
      "source": [
        "Some `tf.Operation`s are **registered as being non-differentiable** and will return `None`. Others have **no gradient registered**.\n",
        "\n",
        "The [tf.raw_ops](https://www.tensorflow.org/api_docs/python/tf/raw_ops) page shows which low-level ops have gradients registered.\n",
        "\n",
        "Если вы попытаетесь взять градиент через операцию с плавающей запятой, для которой не зарегистрирован градиент, лента выдаст ошибку вместо того, чтобы молча вернуть `None` . Таким образом вы узнаете, что что-то пошло не так.\n",
        "\n",
        "Например, функция `tf.image.adjust_contrast` обертывает [raw_ops.AdjustContrastv2,](https://www.tensorflow.org/api_docs/python/tf/raw_ops#.AdjustContrastv2) который может иметь градиент, но градиент не реализован:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "HSb20FXc_V0U"
      },
      "outputs": [

      ],
      "source": [
        "image = tf.Variable([[[0.5, 0.0, 0.0]]])\n",
        "delta = tf.Variable(0.1)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  new_image = tf.image.adjust_contrast(image, delta)\n",
        "\n",
        "try:\n",
        "  print(tape.gradient(new_image, [image, delta]))\n",
        "  assert False   # This should not happen.\n",
        "except LookupError as e:\n",
        "  print(f'{type(e).__name__}: {e}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pDoutjzATiEm"
      },
      "source": [
        "Если вам нужно различать эту операцию, вам нужно либо реализовать градиент и зарегистрировать его (используя `tf.RegisterGradient` ), либо повторно реализовать функцию, используя другие операции."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GCTwc_dQXp2W"
      },
      "source": [
        "## Нули вместо None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TYDrVogA89eA"
      },
      "source": [
        "В некоторых случаях было бы удобно получить 0 вместо `None` для несвязанных градиентов. Вы можете решить, что возвращать, когда у вас есть несвязанные градиенты, используя аргумент `unconnected_gradients` :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "U6zxk1sf9Ixx"
      },
      "outputs": [

      ],
      "source": [
        "x = tf.Variable([2., 2.])\n",
        "y = tf.Variable(3.)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  z = y**2\n",
        "print(tape.gradient(z, x, unconnected_gradients=tf.UnconnectedGradients.ZERO))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Tce3stUlHN0L"
      ],
      "name": "autodiff.ipynb",
      "private_outputs": true,
      "provenance": [

      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
